
import pandas as pd
import numpy as np

fb = pd.read_csv('Path/to/the/Dataset/Facebook.csv')

# Part A --> Create data subsets

fb1 = fb[['Page total likes', 'Category', 'Post Month', 'Post Weekday']].loc[0:15]
print(fb1)

fb2 = fb[['Page total likes', 'Category', 'Post Month', 'Post Weekday']].loc[16:30]
print(fb2)

fb3 = fb[['Page total likes', 'Category', 'Post Month', 'Post Weekday']].loc[31:50]
print(fb3)

# Part B --> Merge Data

merging = pd.concat([fb1, fb2, fb3])
print(merging)


# Part C --> Sort Data

sort_values = fb.sort_values('Page total likes', ascending = False)
print(sort_values)

# Part D --> Transpose of data

print(fb.transpose())


# Part E --> Shaping Reshaping

shaping = fb.shape
print(shaping)

pivot_table = pd.pivot_table(fb, index = ['Type', 'Category'], values = 'comment')
print(pivot_table)

# reshaping using array

reshaping_arr = np.array([1, 2, 3, 4, 5, 6])
reshaping_arr.reshape(3, 2)

#now start of second
#
#
#
import os 
import pandas as pd 
import numpy as np
os.chdir(r"C:\\Users\\rohit\\OneDrive\\Desktop\\DSBDA")

#air_q = pd.read_csv('airquality.csv',index_col=0)
#pd.head()

aq = pd.read_csv(r'C:\Users\rohit\OneDrive\Desktop\DSBDA\airquality.csv')
hrt = pd.read_csv(r'C:\Users\rohit\OneDrive\Desktop\DSBDA\heart.csv')

print(aq.head())
print(hrt.head())

print(aq.isnull().sum())  #to check if any null value



# Part A --> Data Cleaning

# Handle missing values
aq['Ozone'].fillna(aq['Ozone'].mean(), inplace=True)
aq['Solar.R'].fillna(aq['Solar.R'].median(), inplace=True)

# Remove duplicates
aq.drop_duplicates(inplace=True)

# Remove outliers
aq = aq[(aq['Ozone'] >= 0) & (aq['Ozone'] <= 200)]

# Standardize the Date column
aq['Day'] = pd.to_datetime(aq['Day'])

# Print cleaned dataset
print(aq.head())



# Part B --> Data Integration

# Concatenate the datasets vertically
merged_data = pd.concat([aq, hrt], axis=0)

# Print the integrated dataset
print(merged_data.head())


# Part C --> Data transformation
print(aq.dtypes)

# Converting datatype explicitly

aq['Ozone'] = aq['Ozone'].astype('object')
print(aq.dtypes)


# Min-Max Normalization
def MinMax_normalize(x):
    return (x - x.min()) / (x.max() - x.min())

# Apply the transformation function to the specified columns
columns_to_normalize = ['Ozone', 'Solar.R', 'Wind', 'Temp', 'Month', 'Day']
aq[columns_to_normalize] = aq[columns_to_normalize].apply(MinMax_normalize)

# Z-Score Normalization
def ZScore_normalize(x):
    return (x - x.mean()) / x.std()

# Apply the transformation function to the specified columns
aq[columns_to_normalize] = aq[columns_to_normalize].apply(ZScore_normalize)

columns_to_normalize = ['Ozone', 'Solar.R', 'Wind', 'Temp', 'Month', 'Day']
aq[columns_to_normalize] = ZScore_normalize(aq[columns_to_normalize])


## get dummies for categorical data

# Create separate arrays for each column
Ozone = [41, 36, 12, 18, 37, 29, 23]
Solar_R = [190, 118, 149, 313, None, 299, 99]
Wind = [7.4, 8.0, 12.6, 11.5, 12.6, 10.9, 13.8]
Temp = [67, 72, 74, 62, 65, 66, 68]
Month = [5, 5, 5, 5, 6, 6, 6]
Day = [1, 2, 3, 4, 5, 6, 7]

# Create a dictionary with column names as keys and corresponding arrays as values
data = {'Ozone': Ozone, 'Solar.R': Solar_R, 'Wind': Wind, 'Temp': Temp, 'Month': Month, 'Day': Day}

# Create the DataFrame
aqdf = pd.DataFrame(data)

# Display the DataFrame
print(aqdf)


# Part D --> Error correcting

# Check unique values in the respective columns
print(np.unique(aq['Ozone']))
print(np.unique(aq['Solar.R']))
print(np.unique(aq['Wind']))
print(np.unique(aq['Temp']))
print(np.unique(aq['Month']))
print(np.unique(aq['Day']))

# Replace inconsistent values with desired format
aq['Ozone'].replace('missing_value', np.nan, inplace=True)  # Replace 'missing_value' with NaN or any appropriate value
aq['Solar.R'].replace('missing_value', np.nan, inplace=True)  # Replace 'missing_value' with NaN or any appropriate value

# Convert columns to numeric data type
aq['Ozone'] = pd.to_numeric(aq['Ozone'])
aq['Solar.R'] = pd.to_numeric(aq['Solar.R'])

# Verify the changes
print(aq['Ozone'].dtype)
print(aq['Solar.R'].dtype)




   #now third ih here

#
#
#
#
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Read the CSV file
cars_data = pd.read_csv('Toyota.csv', index_col=0, na_values=['??', '????'])

# Drop rows with missing values
cars_data.dropna(axis=0, inplace=True)

# Scatter Plot
plt.scatter(cars_data['Age'], cars_data['Price'], c='blue')
plt.title("Scatter Plot Car Price vs Age")
plt.xlabel('Age in months')
plt.ylabel('Price in Dollars')
plt.show()

# Histogram
plt.hist(cars_data['KM'], color='blue', edgecolor='white', bins=5)
plt.title("Histogram of Kilometer run")
plt.xlabel('Kilometers')
plt.ylabel('Frequency')
plt.show()

# Bar Plot
fuelTypes = ('Petrol', 'Diesel', 'CNG')
counts = cars_data['FuelType'].value_counts()
index = np.arange(len(fuelTypes))

plt.bar(index, counts, color=['red', 'green', 'cyan'])
plt.title("Bar Plot of Fuel Type")
plt.xlabel('Fuel Used')
plt.ylabel('Frequency')
plt.xticks(index, fuelTypes, rotation=90)
plt.show()

# Scatter Plot using seaborn
sns.set(style='darkgrid')
sns.regplot(x=cars_data['Age'], y=cars_data['Price'])
plt.show()

# Histogram using seaborn
sns.displot(cars_data['Age'])
plt.show()

# Bar Plot using seaborn
sns.countplot(x='FuelType', data=cars_data)
plt.show()

# Box Plot using seaborn
sns.boxplot(y=cars_data['Price'])
plt.show()
sns.boxplot(y=cars_data['Price'], x=cars_data['FuelType'])
plt.show()










##hadoop
#
#
#
#
Steps for running Word count- Map-Reduce example-
1. format hadoop namenode

home/hduser: ~$ hadoop namenode -format
Note that hadoop namenode -format command should be executed once before we start
using Hadoop. If this command is executed again after Hadoop has been used, it'll destroy all the
data on the Hadoop file system.
1. Create directory on home WordCountAss – place WordCount.java to this directory
2. Create directory on home WordCountAss / input – to place a file ‘input.text’
3. Create directory on home WordCountAss / Wc_Classes – to place all class files of compiled
WordCount.java
2. start all dfs and yarn deomans
home/hduser: ~$ start-all.sh
home/hduser: ~$ jps
[ Make sure that all dfs and yarn deomans are running, if not stop all deomans and then remove
and recreate datanode and namenode diecrtories in hadoop space]
3. Set classpath
home/hduser: ~$ export HADOOP_CLASSPATH=$(hadoop classpath)
home/hduser: ~$ echo $HADOOP_CLASSPATH
4. Create directotries on hadoop file system
home/hduser: ~$ hadoop dfs -mkdir /wc
home/hduser: ~$ hadoop dfs -mkdir /wc /wc/input
5. Place input.txt on hadoop file system
home/hduser: ~$ hadoop fs -put '/home/hduser/WordCountAss/input/input.txt' /wc/input
6. Check on namenode web interface if file is copied on hadoop file system
7. Compile WordCount.java
home/hduser: ~$ cd WordCountAss
home/hduser/WordCountAss: ~$ javac -classpath
${HADOOP_CLASSPATH}-d'/home/hduser/WordCountAss/WC_classes'
'/home/hduser/WC/WordCount.java'
8. Create jar file
home/hduser/WordCountAss: ~$ jar cvf wc2.jar-C WC_classes/.
9. execute jar.
home/hduser/WordCountAss: ~$ hadoop jar /home/hduser/WordCountAss/wc2.jar
WordCount /wc/input /wc/output
10. Successful execution, observe the output at hadoop file system and copy the the same to view
on terminal.
home/hduser/WordCountAss: ~ $ hadoop dfs -cat /wc/output/*
11. stop all deamons
home/hduser: ~$ stop-all.sh


*****************


The provided code is a step-by-step guide for running a Word Count Map-Reduce example in Hadoop. Let's go through each step to understand what it does:

    Format the Hadoop NameNode:
    This step formats the Hadoop NameNode, which prepares it for use. It is a one-time command that initializes the file system metadata and directory structure.

    Create the necessary directories:
    Here, you create three directories: "WordCountAss" (the main directory), "WordCountAss/input" (to store the input file), and "WordCountAss/WC_Classes" (to store the compiled class files of the WordCount program).

    Start all DFS and YARN daemons:
    This step starts the DFS (Distributed File System) and YARN (Yet Another Resource Negotiator) daemons. These are essential components of Hadoop for distributed file storage and resource management, respectively.

    Set the classpath:
    The classpath is set to include the Hadoop libraries. This ensures that the Hadoop commands and classes are accessible during compilation and execution.

    Create directories on the Hadoop file system:
    Use the Hadoop command "hadoop dfs -mkdir" to create two directories: "/wc" and "/wc/input" on the Hadoop file system. These directories will be used to store the input and output files for the Word Count example.

    Place the input file on the Hadoop file system:
    The input file, "input.txt," is copied from the local file system to the Hadoop file system using the "hadoop fs -put" command. It is stored in the "/wc/input" directory.

    Check the Namenode web interface:
    This step suggests checking the Namenode web interface to verify if the input file has been successfully copied to the Hadoop file system. The Namenode web interface provides a graphical representation of the Hadoop cluster's status and file system.

    Compile WordCount.java:
    The WordCount.java file is compiled using the "javac" command. The "-classpath" option ensures that the Hadoop libraries are included during compilation. The compiled class files are placed in the "WordCountAss/WC_Classes" directory.

    Create a JAR file:
    A JAR (Java Archive) file is created using the "jar" command. It packages all the class files in the "WC_Classes" directory into a single JAR file named "wc2.jar."

    Execute the JAR file:
    The JAR file is executed using the "hadoop jar" command. The WordCount class is specified as the main class, followed by the input and output paths ("/wc/input" and "/wc/output" respectively).

    View the output:
    Once the execution is complete, the output can be observed on the Hadoop file system using the "hadoop dfs -cat" command. The output files are located in the "/wc/output" directory.

    Stop all daemons:
    Finally, all the Hadoop daemons are stopped using the "stop-all.sh" script. This ensures that the Hadoop cluster is cleanly shut down.

By following these steps, you can run the Word Count Map-Reduce example in Hadoop and obtain the word count output.


********* j code

import java.io.IOException;
import java.util.StringTokenizer;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.Reducer;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;

public class WordCount {

  public static class TokenizerMapper
       extends Mapper<Object, Text, Text, IntWritable>{

    private final static IntWritable one = new IntWritable(1);
    private Text word = new Text();

    public void map(Object key, Text value, Context context
                    ) throws IOException, InterruptedException {
      StringTokenizer itr = new StringTokenizer(value.toString());
      while (itr.hasMoreTokens()) {
        word.set(itr.nextToken());
        context.write(word, one);
      }
    }
  }

  public static class IntSumReducer
       extends Reducer<Text,IntWritable,Text,IntWritable> {
    private IntWritable result = new IntWritable();

    public void reduce(Text key, Iterable<IntWritable> values,
                       Context context
                       ) throws IOException, InterruptedException {
      int sum = 0;
      for (IntWritable val : values) {
        sum += val.get();
      }
      result.set(sum);
      context.write(key, result);
    }
  }

  public static void main(String[] args) throws Exception {
    Configuration conf = new Configuration();
    Job job = Job.getInstance(conf, "word count");
    job.setJarByClass(WordCount.class);
    job.setMapperClass(TokenizerMapper.class);
    job.setCombinerClass(IntSumReducer.class);
    job.setReducerClass(IntSumReducer.class);
    job.setOutputKeyClass(Text.class);
    job.setOutputValueClass(IntWritable.class);
    FileInputFormat.addInputPath(job, new Path(args[0]));
    FileOutputFormat.setOutputPath(job, new Path(args[1]));
    System.exit(job.waitForCompletion(true) ? 0 : 1);
  }
}






#oral
#
Tableau is a powerful and widely used data visualization and business intelligence tool developed by Tableau Software. It allows users to connect to various data sources, create interactive visualizations, and share insights with others. Tableau provides a user-friendly interface and offers a range of features that make it easy to explore and analyze data.


1D Visualization:
A 1D visualization represents data along a single dimension, typically using a one-dimensional plot.
histogram
bar graph
line graph
step graph

Linear Visualization:
Linear visualization typically refers to the representation of data on a linear scale or axis. It implies a continuous progression of values without any breaks or gaps.

2d visualization

planar visualization

what is cluster?   
variance
normalization --> 
zero
std deviation
mean
median
PetalLengthCmsepal
pandas
numpy
matplot
seaborn
cleaning
integration
normalize
minmax normalize
shape  --> The "shape" method is used to determine the dimensions or shape of an array or dataframe. 
reshape --> The "reshape" method is used to change the dimensions or shape of an array.


hadoop --> Hadoop is an open-source framework designed to store and process large amounts of data 

(HDFS): It is a distributed file system that provides high-throughput access to data across a cluster of machines. HDFS breaks large files into smaller blocks and replicates them across multiple machines, ensuring fault tolerance and data availability.

MapReduce: It is a programming model and processing engine for distributed data processing in Hadoop.  The processing logic consists of two phases: the "Map" phase, where data is divided into smaller chunks and processed independently, and the "Reduce" phase, where the intermediate results from the Map phase are combined to produce the final output.

YARN (Yet Another Resource Negotiator): It is a resource management framework in Hadoop that allows multiple data processing engines to run on the same cluster. 

The NameNode is the main node and it doesn’t store the actual data. It contains metadata, just like a log file or you can say as a table of content. Therefore, it requires less storage and high computational resources.

all your data is stored on the DataNodes and hence it requires more storage resources. These DataNodes are commodity hardware (like your laptops and desktops) in the distributed environment. 

PIG has two parts: Pig Latin, the language and the pig runtime, for the execution environment. You can better understand it as Java and JVM.

HIVE is a data warehousing component which performs reading, writing and managing large data sets in a distributed environment using SQL-like interface.
HIVE + SQL = HQL         The query language of Hive is called Hive Query Language(HQL), which is very similar like SQL.

Mahout provides an environment for creating machine learning applications which are scalable. 

Apache Spark is a framework for real time data analytics in a distributed computing environment.The Spark is written in Scala.It executes in-memory computations to increase speed of data processing over Map-Reduce. It is 100x faster than Hadoop for large scale data processing by exploiting in-memory computations and other optimizations. 

HBase is an open source, non-relational distributed database. In other words, it is a NoSQL database. It supports all types of data and that is why, it’s capable of handling anything and everything inside a Hadoop ecosystem.

Apache Drill is used to drill into any kind of data. It’s an open source application which works with distributed environment to analyze large data sets.

The Flume is a service which helps in ingesting unstructured and semi-structured data into HDFS. It gives us a solution which is reliable and distributed and helps us in collecting, aggregating and moving large amount of data sets.

Flume only ingests unstructured data or semi-structured data into HDFS.While Sqoop can import as well as export structured data from RDBMS or Enterprise data warehouses to HDFS or vice versa.

What is the purpose of the Hadoop NameNode format command, and why is it important to execute it before using Hadoop?
ChatGPT

The purpose of the Hadoop NameNode format command is to initialize the file system metadata stored by the NameNode. When you run the command hadoop namenode -format, it creates an empty file system image and initializes the directory structure for storing data blocks and metadata.

How did you set the classpath for Hadoop? Why is it necessary, and what does it enable?
Setting the classpath for Hadoop is necessary to ensure that the Hadoop libraries and dependencies are accessible to the Java runtime environment when compiling and running Hadoop programs.

JDK (Java Development Kit): JDK is a software development kit that provides tools and libraries for developing Java applications. It includes the necessary components to compile, debug, and run Java programs.

JVM (Java Virtual Machine): JVM is an integral part of the Java platform. It is a virtual machine that executes Java bytecode, which is the compiled form of Java source code. 	


****************


The steps provided outline the process for running a Word Count Map-Reduce example on Hadoop. Here's a breakdown of each step:

    Format Hadoop Namenode: Run the command hadoop namenode -format to initialize the Hadoop file system. Note that this command should be executed only once before using Hadoop.

    Create Directories: Create the necessary directories on your local machine. Create a directory called "WordCountAss" and place the "WordCount.java" file in it. Create a subdirectory called "input" inside "WordCountAss" to store the input file. Additionally, create a subdirectory called "Wc_Classes" to store the compiled class files.

    Start Hadoop Daemons: Start all the Hadoop distributed file system (DFS) and Yet Another Resource Negotiator (YARN) daemons using the start-all.sh script. Use the jps command to ensure that all the daemons are running.

    Set Classpath: Set the classpath environment variable to include the Hadoop classpath using the command export HADOOP_CLASSPATH=$(hadoop classpath). Verify the classpath by echoing the variable.

    Create Directories on Hadoop File System: Use the command hadoop dfs -mkdir to create directories on the Hadoop file system. Create the directories /wc and /wc/input to store the input and output files.

    Place Input File on Hadoop File System: Use the command hadoop fs -put to copy the input file (input.txt) from your local machine to the Hadoop file system under the /wc/input directory.

    Compile WordCount.java: Change to the "WordCountAss" directory and compile the "WordCount.java" file using the javac command. Specify the classpath and output directory.

    Create Jar File: Create a JAR file using the jar command. Specify the location of the class files.

    Execute Jar: Run the Word Count example by executing the JAR file using the hadoop jar command. Provide the input and output directories as arguments.

    Check Output: After successful execution, check the output on the Hadoop file system using the hadoop dfs -cat command. This will display the output on the terminal.

    Stop Hadoop Daemons: Use the stop-all.sh script to stop all the Hadoop daemons.

Please note that the steps assume a Unix-like environment, and you may need to adjust the commands accordingly if you are using a different operating system.
